{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Image datasetset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the .npz file\n",
    "data = np.load('dataset_features.npz')\n",
    "\n",
    "# List all arrays within the .npz file\n",
    "print(data.files)\n",
    "\n",
    "# Access individual arrays by their names\n",
    "X_train = data['trainset_features']\n",
    "y_train = data['trainset_labels']\n",
    "\n",
    "X_val = data['validset_features']\n",
    "y_val = data['validset_labels']\n",
    "\n",
    "X_test = data['testset_features']\n",
    "y_test = data['testset_labels']\n",
    "\n",
    "class_labels = data['class_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_class(y):\n",
    "    return np.argmax(y,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a k-NN pipeline\n",
    "logreg_pipe = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()), \n",
    "     (\"logreg\", LogisticRegression(multi_class='multinomial', solver='saga', penalty='none'))]\n",
    ")\n",
    "\n",
    "# Fit it to train data\n",
    "logreg_pipe.fit(X_train, decode_class(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on train, validation and test sets\n",
    "print('Model Accuracy:')\n",
    "print(f'On train set: {logreg_pipe.score(X_train, decode_class(y_train)):.3f}')\n",
    "print(f'On valid set: {logreg_pipe.score(X_val, decode_class(y_val)):.3f}')\n",
    "print(f'On test  set: {logreg_pipe.score(X_test, decode_class(y_test)):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of model coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = logreg_pipe.named_steps['logreg'].coef_ \n",
    "coefficients.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the coefficients as a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.heatmap(np.abs(coefficients), annot=False, cbar=True)\n",
    "ax.set_yticklabels(class_labels, rotation=0)\n",
    "ax.set_xlabel(\"Feature Index\")\n",
    "plt.title(\"Logistic Regression Coefficients Heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the indices of the top 5 largest coefficients (absolute values) for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "top_features = {}\n",
    "for class_index, class_coefficients in enumerate(coefficients):\n",
    "    # Get the indices of the top 5 largest coefficients for the current class\n",
    "    largest_indices = np.argsort(-np.abs(class_coefficients))[:5]\n",
    "    top_features[f\"{class_labels[class_index]}\"] = largest_indices\n",
    "\n",
    "\n",
    "top_features_df = pd.DataFrame.from_dict(top_features, orient='index', columns=[f\"Feature {i+1}\" for i in range(5)])\n",
    "print(\"Top 5 Largest Coefficients for Each Class (Feature Indices):\")\n",
    "print(top_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_crossval = np.concatenate((X_train, X_val), axis=0)\n",
    "y_crossval = decode_class(np.concatenate((y_train, y_val), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define logistic regression model with L2 regularization\n",
    "log_reg = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, multi_class='multinomial')\n",
    "\n",
    "# Define grid of regularization strengths to test\n",
    "param_grid = {\n",
    "    'C': np.logspace(-6, 3, 20)  # Test values from 10^-4 to 10^4\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',  # Metric for evaluating models\n",
    "    cv=5,                # 5-fold cross-validation\n",
    "    n_jobs=-1,           # Use all available processors\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_crossval, y_crossval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results from the GridSearchCV object\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Select relevant columns for interpretation\n",
    "results_df = results[\n",
    "    [\n",
    "        'param_C',  # Regularization strength parameter\n",
    "        'mean_train_score',  # Mean training score across folds\n",
    "        'std_train_score',  # Standard deviation of training score across folds\n",
    "        'mean_test_score',  # Mean validation (test) score across folds\n",
    "        'std_test_score'  # Standard deviation of validation score across folds\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Sort by the validation score for better interpretability\n",
    "results_df = results_df.sort_values(by='mean_test_score', ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **mean_train_score** and **std_train_score** focus on the model's fit to the training data.\n",
    "- **mean_test_score** and **std_test_score** assess the model's ability to generalize to unseen data.\n",
    "    \n",
    "    Consistency and Stability:\n",
    "        Low standard deviations (**std_train_score** and **std_test_score**) indicate consistent performance.\n",
    "        High mean validation scores with low standard deviations are desirable for a robust, well-generalizing model.\n",
    "\n",
    "These metrics help diagnose overfitting, underfitting, or data-related issues during hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have metrics recorded during training, such as accuracy or loss\n",
    "# Example data (replace with actual metrics from your training process)\n",
    "\n",
    "results_df = results_df.sort_values(by='param_C')\n",
    "\n",
    "param_c = results_df['param_C'].tolist()\n",
    "train_scores_mean = results_df['mean_train_score'].to_numpy()\n",
    "val_scores_mean = results_df['mean_test_score'].to_numpy()\n",
    "train_scores_std = results_df['std_train_score'].to_numpy()\n",
    "val_scores_std = results_df['std_test_score'].to_numpy()\n",
    "# Replace with validation accuracies\n",
    "\n",
    "# Plot the curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(param_c, train_scores_mean, label=\"Training Accuracy\", marker=\"o\")\n",
    "plt.plot(param_c, val_scores_mean, label=\"Validation Accuracy\", marker=\"o\")\n",
    "plt.fill_between(param_c, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2)\n",
    "plt.fill_between(param_c, val_scores_mean - val_scores_std, val_scores_mean + val_scores_std, alpha=0.2)\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"C Parameter\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that for values of C > 0.001 the model starts to overfitting and not gaining anymore capability to generalize. This is indicated by a gap showing in the model accuracy of the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a k-NN pipeline\n",
    "logreg_pipe_tuned = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()), \n",
    "     (\"logreg\", LogisticRegression(multi_class='multinomial', solver='saga', penalty='none', C=0.005))]\n",
    ")\n",
    "\n",
    "# Fit it to train data\n",
    "logreg_pipe.fit(X_train, decode_class(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy on train, validation and test sets\n",
    "print('Model Accuracy:')\n",
    "print(f'On train set: {logreg_pipe_tuned.score(X_train, decode_class(y_train)):.3f}')\n",
    "print(f'On valid set: {logreg_pipe_tuned.score(X_val, decode_class(y_val)):.3f}')\n",
    "print(f'On test  set: {logreg_pipe_tuned.score(X_test, decode_class(y_test)):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
