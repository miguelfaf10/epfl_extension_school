{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Image datasetset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the .npz file\n",
    "data = np.load('dataset_features.npz')\n",
    "\n",
    "# List all arrays within the .npz file\n",
    "print(data.files)\n",
    "\n",
    "# Access individual arrays by their names\n",
    "X_train = data['trainset_features']\n",
    "y_train = data['trainset_labels']\n",
    "\n",
    "X_val = data['validset_features']\n",
    "y_val = data['validset_labels']\n",
    "\n",
    "X_test = data['testset_features']\n",
    "y_test = data['testset_labels']\n",
    "\n",
    "class_labels = data['class_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from util import decode_class\n",
    "\n",
    "# Create a k-NN pipeline\n",
    "logreg_pipe = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()), \n",
    "     (\"logreg\", LogisticRegression(multi_class='multinomial', solver='saga', penalty='none'))]\n",
    ")\n",
    "\n",
    "# Fit it to train data\n",
    "logreg_pipe.fit(X_train, decode_class(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy score on train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Accuracy:')\n",
    "acc_train = logreg_pipe.score(X_train, decode_class(y_train))\n",
    "print(f'On train set: {acc_train:.3f}')\n",
    "acc_val = logreg_pipe.score(X_val, decode_class(y_val))\n",
    "print(f'On valid set: {acc_val:.3f}')\n",
    "acc_test = logreg_pipe.score(X_test, decode_class(y_test))\n",
    "print(f'On test  set: {acc_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Classification report\n",
    "y_test_preds = logreg_pipe.predict(X_test)\n",
    "\n",
    "print(classification_report(y_true=decode_class(y_test), y_pred=y_test_preds, target_names=class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model coefficients visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = logreg_pipe.named_steps['logreg'].coef_ \n",
    "coefficients.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizations of the coefficients as a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.heatmap(np.abs(coefficients), annot=False, cbar=True)\n",
    "ax.set_yticklabels(class_labels, rotation=0)\n",
    "ax.set_xlabel(\"Feature Index\")\n",
    "plt.title(\"Logistic Regression Coefficients Heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look for the indices of the top 5 largest coefficients (absolute values) for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "top_features = {}\n",
    "for class_index, class_coefficients in enumerate(coefficients):\n",
    "    \n",
    "    # Get the indices of the top 5 largest coefficients for the current class\n",
    "    largest_indices = np.argsort(-np.abs(class_coefficients))[:5]\n",
    "    top_features[f\"{class_labels[class_index]}\"] = largest_indices\n",
    "\n",
    "\n",
    "top_features_df = pd.DataFrame.from_dict(top_features, orient='index', columns=[f\"Feature {i+1}\" for i in range(5)])\n",
    "print(\"Top 5 Largest Coefficients for Each Class (Feature Indices):\")\n",
    "print(top_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation:__\n",
    "\n",
    "- The coefficients obtained in Task 1 were:\n",
    "\n",
    "    bike:        **[148, 801, 183, 1094, 54]**  \n",
    "    car:         **[1098, 291, 183, 660, 257]**  \n",
    "    motorcycle:  **[1043, 505, 898, 1122, 1120]**  \n",
    "    other:       **[580, 529, 734, 279, 411]**  \n",
    "    truck:       **[1051, 335, 580, 714, 1022]**  \n",
    "    van:         **[893, 466, 1113, 1104, 1022]**  \n",
    "\n",
    "- We observe that except for a few exceptions most high-level features are different. The shared coefficients are:\n",
    "    - Bike: **801**  \n",
    "    - Car: **291**  \n",
    "    - Motorcycle: **1122**  \n",
    "    - Other: **None**  \n",
    "    - Truck: **None**\n",
    "    - Van: **None**\n",
    "\n",
    "- This is not really consistent with the exploration in Task 1, although the categories for which there's a common feature are the ones with more samples and slightly better classification scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll apply l2-regularization to the model and fine-tune the parameter (C = 1/lambda) that is the inverse of the strengh of the regularization term in the cost funtion. \n",
    "\n",
    "We first start my merging our train and validation sets to then apply cross-validation technique during the parameter grid-search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_crossval = np.concatenate((X_train, X_val), axis=0)\n",
    "y_crossval = decode_class(np.concatenate((y_train, y_val), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define logistic regression model with L2 regularization\n",
    "log_reg = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000, multi_class='multinomial')\n",
    "\n",
    "# Define grid of regularization strengths to test\n",
    "param_grid = {\n",
    "    'C': np.logspace(-6, 3, 20)  # Test values from 10^-4 to 10^4\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',  # Metric for evaluating models\n",
    "    cv=5,                # 5-fold cross-validation\n",
    "    n_jobs=-1,           # Use all available processors\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_crossval, y_crossval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results from the GridSearchCV object\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Select relevant columns for interpretation\n",
    "results_df = results[\n",
    "    [\n",
    "        'param_C',  \n",
    "        'mean_train_score',\n",
    "        'std_train_score', \n",
    "        'mean_test_score', \n",
    "        'std_test_score'  \n",
    "    ]\n",
    "]\n",
    "\n",
    "# Sort by the validation score for better interpretability\n",
    "results_df = results_df.sort_values(by='mean_test_score', ascending=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **mean_train_score** and **std_train_score**: refer to average and standard deviation of the model accuracy obtained over the 5-fold (5x 4/5 of total data) of the train sub-sets derived from the merged train+val dataset.\n",
    "- **mean_test_score** and **std_test_score**: the same as before but this it represents the score in the 5-fold (5x 1/5 of total data) validation sets are created.\n",
    "    \n",
    "- Low standard deviations (**std_train_score** and **std_test_score**) indicate consistent performance.\n",
    "\n",
    "- High mean validation scores with low standard deviations are desirable for a robust,  well-generalizing model.\n",
    "\n",
    "- These metrics help diagnose overfitting, underfitting, or data-related issues during hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have metrics recorded during training, such as accuracy or loss\n",
    "# Example data (replace with actual metrics from your training process)\n",
    "\n",
    "results_df = results_df.sort_values(by='param_C')\n",
    "\n",
    "param_c = results_df['param_C'].tolist()\n",
    "train_scores_mean = results_df['mean_train_score'].to_numpy()\n",
    "val_scores_mean = results_df['mean_test_score'].to_numpy()\n",
    "train_scores_std = results_df['std_train_score'].to_numpy()\n",
    "val_scores_std = results_df['std_test_score'].to_numpy()\n",
    "# Replace with validation accuracies\n",
    "\n",
    "# Plot the curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(param_c, train_scores_mean, label=\"Training Accuracy\", marker=\"o\")\n",
    "plt.plot(param_c, val_scores_mean, label=\"Validation Accuracy\", marker=\"o\")\n",
    "plt.fill_between(param_c, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2)\n",
    "plt.fill_between(param_c, val_scores_mean - val_scores_std, val_scores_mean + val_scores_std, alpha=0.2)\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "plt.xlabel(\"C Parameter\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that for values of C > 0.001 the model starts to overfitting and not gaining anymore capability to generalize. This is indicated by a gap showing in the model accuracy of the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logreg_pipe_tuned = grid_search.best_estimator_\n",
    "logreg_pipe_tuned = logreg_pipe.set_params(logreg__C=0.005)\n",
    "\n",
    "# Fit it to train data\n",
    "logreg_pipe_tuned.fit(X_train, decode_class(y_train))\n",
    "\n",
    "# Check accuracy on training set\n",
    "acc_train = logreg_pipe_tuned.score(X_train, decode_class(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of model with tuned l2-regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model Accuracy:')\n",
    "acc_train = logreg_pipe_tuned.score(X_train, decode_class(y_train))\n",
    "print(f'On train set: {acc_train:.3f}')\n",
    "acc_val = logreg_pipe_tuned.score(X_val, decode_class(y_val))\n",
    "print(f'On valid set: {acc_val:.3f}')\n",
    "acc_test = logreg_pipe_tuned.score(X_test, decode_class(y_test))\n",
    "print(f'On test  set: {acc_test:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observation:__\n",
    "\n",
    "- The model accuracy on the test set with the optimized l2-regularization is exactly the same as what was obtained without regularization: accuracy = 0.96\n",
    "- As can be seen from the training and validation curves, the regularization term doesn't affect the already very high model performance, except when the l2-term becomes excidengly high for C<0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we save the accuracy results on the test data set in the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "\n",
    "with open('model_accuracy.pickle', 'rb') as file:\n",
    "    results_acc = pickle.load(file)\n",
    "    \n",
    "with open('model_accuracy.pickle', 'wb') as file:\n",
    "    results_acc.loc[len(results_acc)] = {'model': 'logistic', 'test_accuracy': acc_test}\n",
    "    pickle.dump(results_acc, file)\n",
    "    \n",
    "print(results_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
