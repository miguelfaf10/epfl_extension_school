{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- From looking at the dataset documentation we observe that many of the categorical features contain a value which can be 'NA' or 'None' string, which indicates that feature is not present. \n",
    "- By default `pd.read_csv` would interpret this as a missing values which is not the case.\n",
    "\n",
    "Solution:\n",
    "- Change the list of strings to be intepreted as missing values, such that only empty string '' is considered as a missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file ='house-prices.csv'\n",
    "df = pd.read_csv(file, na_values = [''],keep_default_na=False)\n",
    "df.drop(['Order', 'PID'], axis=1, inplace=True)\n",
    "\n",
    "file ='house-prices-test.csv'\n",
    "df_test = pd.read_csv(file, na_values = [''],keep_default_na=False)\n",
    "df_test_pid = df_test['PID']\n",
    "df_test.drop(['Order','PID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- There are 80 features and 2430 entries\n",
    "- We observe there's a mix of categorical and numerical features\n",
    "\n",
    "Solution:\n",
    "- From reading the documentation we can know which features should correspond to a certa type ('nominal', 'ordinal', 'discrete' and 'continuous')\n",
    "- Change the datatypes of the read dataframe to be compatible with the know variable types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of nominal features\n",
    "nominal_features = [\n",
    "    \"MS SubClass\", \"MS Zoning\", \"Street\", \"Alley\", \"Land Contour\",\n",
    "    \"Lot Config\", \"Neighborhood\", \"Condition 1\", \"Condition 2\", \"Bldg Type\",\n",
    "    \"House Style\", \"Roof Style\", \"Roof Matl\", \"Exterior 1st\", \"Exterior 2nd\",\n",
    "    \"Mas Vnr Type\", \"Foundation\", \"Heating\", \"Central Air\", \"Garage Type\",\n",
    "    \"Misc Feature\", \"Sale Type\", \"Sale Condition\"\n",
    "]\n",
    "\n",
    "ordinal_features = [\n",
    "    \"Lot Shape\", \"Utilities\", \"Land Slope\", \"Overall Qual\", \"Overall Cond\",\n",
    "    \"Exter Qual\", \"Exter Cond\", \"Bsmt Qual\", \"Bsmt Cond\", \"Bsmt Exposure\",\n",
    "    \"BsmtFin Type 1\", \"BsmtFin Type 2\", \"Heating QC\", \"Electrical\", \n",
    "    \"Kitchen Qual\", \"Functional\", \"Fireplace Qu\", \"Garage Finish\", \"Garage Qual\",\n",
    "    \"Garage Cond\", \"Paved Drive\", \"Pool QC\", \"Fence\"\n",
    "]\n",
    "\n",
    "discrete_features = [\n",
    "    \"Bsmt Full Bath\", \"Bsmt Half Bath\", \"Full Bath\", \"Half Bath\",\n",
    "    \"Bedroom AbvGr\", \"Kitchen AbvGr\", \"TotRms AbvGrd\", \"Fireplaces\", \"Garage Yr Blt\",\n",
    "    \"Garage Cars\", \"Mo Sold\", \"Yr Sold\", \"Year Built\", \"Year Remod/Add\"\n",
    "]\n",
    "continuous_features = [\n",
    "    \"BsmtFin SF 1\", \"BsmtFin SF 2\", \"Bsmt Unf SF\", \"Total Bsmt SF\",\n",
    "    \"1st Flr SF\", \"2nd Flr SF\", \"Low Qual Fin SF\", \"Gr Liv Area\",\n",
    "    \"Garage Area\", \"Wood Deck SF\", \"Open Porch SF\", \"Enclosed Porch\",\n",
    "    \"3Ssn Porch\", \"Screen Porch\", \"Pool Area\", \"Misc Val\",\n",
    "    \"Lot Frontage\", \"Lot Area\", \"Mas Vnr Area\"\n",
    "]\n",
    "\n",
    "for col in nominal_features:\n",
    "        df[col] = df[col].astype('object')\n",
    "        df_test[col] = df_test[col].astype('object')\n",
    "\n",
    "for col in ordinal_features:\n",
    "        df[col] = df[col].astype('object')\n",
    "        df_test[col] = df_test[col].astype('object')\n",
    "\n",
    "df['Overall Cond'] = df['Overall Cond'].astype('str')\n",
    "df['Overall Qual'] = df['Overall Qual'].astype('str')\n",
    "\n",
    "df_test['Overall Cond'] = df_test['Overall Cond'].astype('str')\n",
    "df_test['Overall Qual'] = df_test['Overall Qual'].astype('str')\n",
    "\n",
    "for col in discrete_features:\n",
    "        df_test[col] = df_test[col].astype('Int64')\n",
    "\n",
    "for col in continuous_features:\n",
    "        df_test[col] = df_test[col].astype('float64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll perform the data cleaning operations treating missing values, outliers and inconsistencies. This will be performed idependently for the different types of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nominal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of ordinal features: {len(nominal_features)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Missing values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total missing values in ordinal features: {df[nominal_features].isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ordinal_missing = df[nominal_features].isna().sum().to_frame(name='total')\n",
    "df_ordinal_missing = df_ordinal_missing[df_ordinal_missing['total']!=0].sort_values('total', ascending=False)\n",
    "df_ordinal_missing['percentage'] = df_ordinal_missing['total']/len(df)*100\n",
    "df_ordinal_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- According to the documentation, all 4 features with missing values contain a category for 'NA' (not available)\n",
    "- From a quick check of the original CSV file, it is highly probable that these missing values should instead be 'NA', corresponding to cases where such a feature is not present\n",
    "\n",
    "Solution:\n",
    "- Replace the missing values with 'NA'\n",
    "- Replace all existing values of 'None' with 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with 'None'\n",
    "for feature in ['Misc Feature', 'Alley', 'Garage Type', 'Mas Vnr Type']:\n",
    "    df[feature] = df[feature].fillna('NA')\n",
    "    df_test[feature] = df_test[feature].fillna('NA')\n",
    "    \n",
    "for feature in nominal_features:\n",
    "    df[feature].replace('None','NA', inplace=True)\n",
    "    df_test[feature].replace('None','NA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in Nominal columns after cleaning: \", df[nominal_features].isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature distribution__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next observe the distribution of all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 5\n",
    "n_rows = 5\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(15, 10))\n",
    "\n",
    "for i, col in enumerate(nominal_features):\n",
    "    r, c = np.divmod(i, n_cols)\n",
    "    df[col].value_counts().plot(kind='bar', ax=axs[r, c], logy=True)\n",
    "    axs[r, c].set_title(col, y=1.0)\n",
    "    axs[r, c].set_xlabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation\n",
    "- For most features there's a a difference in 1 or more orders of magnitude between most and least frequent values\n",
    "\n",
    "Solution\n",
    "- As part of Outlier removal, we may decide to remove the entries whose values in a certain feature correpond to the least represented categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Outlier removal__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective for this model is to predict the Sale Price of \"normal\" residential homes, without considering very rare or specific types of propetries that are represented by very few entries in the dataset.\n",
    "Hence, our strategy for outlier removal will be to remove entries which have categories or range of values that fall outside of the more populated categories or range of values.\n",
    "As said, these may represent real property purchases, but they like are properties other than \"normal\" residential homes. For example, there's a single entry for which the zoning classification \"MS Zoning\" is Industrial \"I\". It makes sense to remove since we aim at modeling only residential properties and not industrial properties.\n",
    "\n",
    "We also stipulated that we can affort to drop a maximum of 5% of the total dataset entries (initially there are 2430), in order to assure we still have a statistically representative dataset for training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Nominal Features: Categories which populated with minimum of entries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Feature\":<20}|{\"Category_MinCount\":<20}|{\"MinCount\":<20}')\n",
    "print(\"-\"*50)\n",
    "for feature in nominal_features:\n",
    "    \n",
    "    # Get the number of entries per category\n",
    "    category_counts = df[feature].value_counts()\n",
    "    min_count_category = category_counts.idxmin()\n",
    "\n",
    "    # Display the counts\n",
    "    print(f'{feature:<20}|{min_count_category:<20}|{min(category_counts):<20}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- For about half of the features there's at least one category populated with just 1 or 2 entries\n",
    "\n",
    "Solution:\n",
    "- We decide to remove the entries for which there's <=2 counts of a given category;\n",
    "- We check how many entries we dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "\n",
    "for feature in nominal_features:\n",
    "\n",
    "    category_counts = df[feature].value_counts()\n",
    "    # Identify categories with counts <= threshold\n",
    "    low_count_categories = category_counts[category_counts <= threshold].index\n",
    "    # Filter out entries with low count categories\n",
    "    df = df[~df[feature].isin(low_count_categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percentage of dropped entries: {(2430-df.shape[0])/2430*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of ordinal features: {len(ordinal_features)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Missing values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total missing values: {df[ordinal_features].isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ordinal_missing = df[ordinal_features].isna().sum().to_frame(name='total')\n",
    "df_ordinal_missing = df_ordinal_missing[df_ordinal_missing['total']!=0].sort_values('total', ascending=False)\n",
    "df_ordinal_missing['percentage'] = df_ordinal_missing['total']/len(df)*100\n",
    "df_ordinal_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- Except for 'Electrical' all other features have a value which corresponds to 'None' according to the dataset doc \n",
    "- From a quick check of the original CSV file, it is highly probable that these missing values should instead be 'None', corresponding to cases where such a feature is not present\n",
    "\n",
    "Solution:\n",
    "- Replace the missing value for 'Electrical' with the mode\n",
    "- Replace the missing values of remaining features with 'NA'\n",
    "- Replace all existing values of 'None' with 'NA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with the mode\n",
    "mode_value = df['Electrical'].mode()[0]\n",
    "df['Electrical'].fillna(mode_value, inplace=True)\n",
    "df_test['Electrical'].fillna(mode_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with 'None'\n",
    "df[ordinal_features] = df[ordinal_features].fillna('NA')\n",
    "df_test[ordinal_features] = df_test[ordinal_features].fillna('NA')\n",
    "\n",
    "df[ordinal_features].replace('None','NA', inplace=True)\n",
    "df_test[ordinal_features].replace('None','NA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in Ordinal columns after cleaning: \", df[ordinal_features].isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature distribution__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next observe the distribution of all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 5\n",
    "n_rows = 5\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(15, 10))\n",
    "\n",
    "for i, col in enumerate(ordinal_features):\n",
    "    r, c = np.divmod(i, n_cols)\n",
    "    df[col].value_counts().plot(kind='bar', ax=axs[r, c], logy=True)\n",
    "    axs[r, c].set_title(col, y=1.0)\n",
    "    axs[r, c].set_xlabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation\n",
    "- For most features there's a a difference in 1 or more orders of magnitude between most and least frequent values\n",
    "- The entries which populated categories with very small counts are good candidates for being considered as outliers in our model as explained before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Outlier removal__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by obtaining the categories with the minimum of entries for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Feature\":<20}|{\"MinCountCategory\":<20}|{\"MinCount\":<20}')\n",
    "print(\"-\"*50)\n",
    "for feature in ordinal_features:\n",
    "    \n",
    "    # Get the number of entries per category\n",
    "    category_counts = df[feature].value_counts()\n",
    "    min_count_category = category_counts.idxmin()\n",
    "\n",
    "    # Display the counts\n",
    "    print(f'{feature:<20}|{min_count_category:<20}|{min(category_counts):<20}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- Again, for about half of the features there's at least one category populated with just 1 or 2 entries\n",
    "\n",
    "Solution:\n",
    "- We decide to remove the entries for which there's <=2 counts of a given category;\n",
    "- We check how many entries we dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "\n",
    "for feature in ordinal_features:\n",
    "\n",
    "    category_counts = df[feature].value_counts()\n",
    "    # Identify categories with counts <= threshold\n",
    "    low_count_categories = category_counts[category_counts <= threshold].index\n",
    "    # Filter out entries with low count categories\n",
    "    df = df[~df[feature].isin(low_count_categories)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percentage of dropped entries: {(2430-df.shape[0])/2430*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of numerical discrete features:', len(discrete_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Missing Values:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total missing values: {df[discrete_features].isna().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_discrete_missing = df[discrete_features].isna().sum().to_frame(name='total')\n",
    "df_discrete_missing = df_discrete_missing[df_discrete_missing['total']!=0].sort_values('total', ascending=False)\n",
    "df_discrete_missing['percentage'] = df_discrete_missing['total']/len(df)*100\n",
    "df_discrete_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- 'Garage Yr Blt' is the feature with more missing values, these correspond to entries where there is no garage. It's not obvious which value to assign in these cases, the feature definition doesn't make sense for these cases.\n",
    "- For 'Bsmt Full Bath', 'Bsmt Half Bath' and 'Garage Cars' the number of missing values is very small\n",
    "\n",
    "Solution: \n",
    "- Check correlation between 'Year Blt' and 'Garage Yr Blt', and if correlation is high we decide to drop the 'Garage Yr Blt' feature altogether\n",
    "- For 'Bsmt Full Bath', 'Bsmt Half Bath' and 'Garage Cars', fill missing values with the mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Garage Yr Blt'].isna()]['Garage Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Correlation of 'Garage Yr Blt' with 'SalePrice': {df['Garage Yr Blt'].corr(df['Year Built'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained before, since the correlation is very high, we decide to drop the feature 'Garage Yr Blt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Garage Yr Blt', axis=1, inplace=True)\n",
    "df_test.drop('Garage Yr Blt', axis=1, inplace=True)\n",
    "discrete_features = list(set(discrete_features) - set(['Garage Yr Blt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values with the mode\n",
    "features = ['Bsmt Full Bath', 'Bsmt Half Bath', 'Garage Cars']\n",
    "\n",
    "for feature in features:\n",
    "    mode_value = df[feature].mode()[0]\n",
    "    df[feature].fillna(mode_value, inplace=True)\n",
    "    df_test[feature].fillna(mode_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Missing values in discrete columns after cleaning: {df[discrete_features].isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature distribution:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 4\n",
    "n_rows = 4\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(10, 8))\n",
    "\n",
    "for i, col in enumerate(discrete_features):\n",
    "    r, c = np.divmod(i, n_cols)\n",
    "    df[col].hist(bins=50, ax=axs[r, c], log=True)\n",
    "    axs[r, c].set_title(col, y=1.0)\n",
    "    axs[r, c].set_xlabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation\n",
    "- For some few features there are values with very few entries (e.g. 'Bsmt full Bath' has very few entries equal to 3)\n",
    "- The entries popupalting these values are good candidates for being considered as outliers in our model as explained before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Outlier removal__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we start by obtaining the categories with the minimum of entries for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"Feature\":<20}|{\"MinValue\":<20}|{\"MinCount\":<20}')\n",
    "print(\"-\"*50)\n",
    "for feature in discrete_features:\n",
    "    \n",
    "    # Get the number of entries per category\n",
    "    category_counts = df[feature].value_counts()\n",
    "    min_count_category = category_counts.idxmin()\n",
    "\n",
    "    # Display the counts\n",
    "    print(f'{feature:<20}|{min_count_category:<20}|{min(category_counts):<20}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- For about half of the features there's at least one value populated with just 1 or 2 entries ('Year Built'=1908, 'Bsmt Full Bath'=3, 'Full Bath'=4)\n",
    "\n",
    "Solution:\n",
    "- We decide to remove the entries for which there's a count <=2 for a given discrete value;\n",
    "- We verify that the total number of dropped entries is still smaller than 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2\n",
    "\n",
    "for feature in discrete_features:\n",
    "\n",
    "    category_counts = df[feature].value_counts()\n",
    "    # Identify categories with counts <= threshold\n",
    "    low_count_categories = category_counts[category_counts <= threshold].index\n",
    "    # Filter out entries with low count categories\n",
    "    df = df[~df[feature].isin(low_count_categories)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percentage of dropped entries: {(2430-df.shape[0])/2430*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of numerical continuous features:', len(continuous_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Missing Values__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Missing values in continuous columns: {df[continuous_features].isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_continuous_missing = df[continuous_features].isna().sum().to_frame(name='total')\n",
    "df_continuous_missing = df_continuous_missing[df_continuous_missing['total']!=0].sort_values('total', ascending=False)\n",
    "df_continuous_missing['percentage'] = df_continuous_missing['total']/len(df)*100\n",
    "df_continuous_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- Feature 'Lot Frontage' has a significant ammount of missing values, these could be from properties with no street connection, but this should be verified\n",
    "- 'Mas Vnr Area' (Masonry veneer area in square feet) with missing values could also be coming from properties with no Masonry veneer surface\n",
    "\n",
    "Solution:\n",
    "- Observe relation between 'Lot Frontage' and 'Lot Config' for properties with no lot frontage.\n",
    "- Check if missing values in 'Mas Vnr Area', also have 'Mas Vnr Type' equal to 'NA'\n",
    "- If previous "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Missing values in 'Lot Frontage':__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Lot Frontage'].isna()]['Lot Config'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most missing values in 'Lot Frontage' correspond to lot configurations - 'Inside' and 'CulDSac' - where it seems likely that there's no street connection. We can reasonably assume that these missing values should correpond to 0 feet of street connection of the property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['Lot Config'].isin(['Inside', 'CulDSac'])) & (df['Lot Frontage'].isna()), 'Lot Frontage'] = 0\n",
    "df_test.loc[(df_test['Lot Config'].isin(['Inside', 'CulDSac'])) & (df_test['Lot Frontage'].isna()), 'Lot Frontage'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remaining missing values in the 'Lot Frontage' column, we will replace them with the median value (more robust to outliers than the mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the remaining missing values in the 'Lot Frontage' column, we will replace them with the median value.\n",
    "median_value = df['Lot Frontage'].median()\n",
    "df['Lot Frontage'].fillna(median_value, inplace=True)\n",
    "df_test['Lot Frontage'].fillna(median_value, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Missing values in 'Mas Vnr Area':__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Mas Vnr Area'].isna()]['Mas Vnr Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All missing values in 'Mas Vnr Area' correspond to entries where 'Mas Vnr Type' is 'NA', hence it makes sense that these values should actually be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Mas Vnr Area'].fillna(0, inplace=True)\n",
    "df_test['Mas Vnr Area'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Missing values in 'Garage Area':__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Garage Area'].isna() | df['Garage Type'].isna() | df['Garage Finish'].isna() | df['Garage Qual'].isna() | df['Garage Cond'].isna()][['Garage Area','Garage Cars','Garage Type','Garage Finish','Garage Qual','Garage Cond']]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the entry with missing value in for 'Garage Area' presents rather inconsistent values for remaining garage variables we decide to drop this entry. For meaning of ordinal features values check previously defined encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(index=1565,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the previous cleanup, the following reduced number of features with missing values remain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Missing values in numerical continuous features after cleaning: {df[continuous_features].isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature Distribution__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 4\n",
    "n_rows = 5\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(10,8))\n",
    "\n",
    "for i, col in enumerate(continuous_features):\n",
    "    r, c = np.divmod(i, n_cols)\n",
    "    df[col].hist(bins=50, ax=axs[r, c])\n",
    "    axs[r, c].set_title(col, y=1.0)\n",
    "    axs[r, c].set_xlabel('')\n",
    "    axs[r, c].set_yscale('log')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- Except for 'Total Bsmt SF', '1st Flr SF', 'Gr Liv Area', 'Garage Area', 'Lot Frontage' and 'Lot Area', all other features have most values equal to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Outlier removal__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- By observing the histogram distributions above, we decide to remove data points which lie far away from the main lobe of datapoints (e.g. '1st Flr SF'>3000). \n",
    "\n",
    "Solution:\n",
    "- Remove the following:\n",
    "    - '1st Flr SF' > 30000\n",
    "    - 'BsmtFin SF 2' > 1250\n",
    "    - 'Wood Deck SF' > 1000\n",
    "    - 'Open Porch SF' > 500\n",
    "    - 'Enclosed Porch' > 500\n",
    "    - 'Misc Val' > 10000\n",
    "    - 'Lot Frontage' > 250\n",
    "    - 'Lot Area' > 100000\n",
    "    - 'Mas Vnr Area' > 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[\n",
    "    (df['1st Flr SF'] <= 3000) &\n",
    "    (df['BsmtFin SF 2'] <= 1250) &\n",
    "    (df['Wood Deck SF'] <= 1000) &\n",
    "    (df['Open Porch SF'] <= 500) &\n",
    "    (df['Enclosed Porch'] <= 500) &\n",
    "    (df['Misc Val'] <= 10000) &\n",
    "    (df['Lot Frontage'] <= 250) &\n",
    "    (df['Lot Area'] <= 100000) &\n",
    "    (df['Mas Vnr Area'] <= 1500)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percentage of dropped entries: {(2430-df.shape[0])/2430*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next plot again the feature distributions, but this time with linear scale on y-axis (counts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 4\n",
    "n_rows = 5\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(10,8))\n",
    "\n",
    "for i, col in enumerate(continuous_features):\n",
    "    r, c = np.divmod(i, n_cols)\n",
    "    df[col].hist(bins=50, ax=axs[r, c])\n",
    "    axs[r, c].set_title(col, y=1.0)\n",
    "    axs[r, c].set_xlabel('')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Obsevation__:\n",
    "- The following features have most entries equal to zero: __'BsmtFin SF 2'__, __'BsmtFin SF 2'__, __'2nd Flr SF'__, __'Low Qual Fin Sf'__, __'Wood Deck SF'__, __'Open Porch SF'__, __'Enclosed Porch'__, __'3Ssn Porch'__, __'Screen Porch'__, __'Pool Area'__, __'Misc Val'__, __'Mas Vnr Area'__\n",
    "- Since having them as continuous features, doesn't add much information, they could be replaced by binary variables, which indicate the presence or not of the given feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inconsistencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll analyse possible incosistencies among the variables related to the following characteritics:\n",
    "- Basement variables\n",
    "- Garage Year Built\n",
    "- Finished vs living areas\n",
    "- 'Year Built' vs 'Year Remod/Add'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Basement variables__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check for possible inconsistencies in Basement related features with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basement_features = ['Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure', 'BsmtFin Type 1', 'BsmtFin SF 1', 'BsmtFin Type 2', 'BsmtFin SF 2', 'Bsmt Unf SF', 'Total Bsmt SF', 'Bsmt Full Bath', 'Bsmt Half Bath']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by looking at entries where at least one feature indicates 'NA', but where this is inconsistent witht he other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = '(`Bsmt Qual` == \"NA\") | (`Bsmt Exposure` == \"NA\") | (`BsmtFin Type 1` == \"NA\") | (`BsmtFin Type 2` == \"NA\")'\n",
    "q2 = '~((`Bsmt Qual` == \"NA\") & (`Bsmt Exposure` == \"NA\") & (`BsmtFin Type 1` == \"NA\") & (`BsmtFin Type 2` == \"NA\") & (`BsmtFin SF 1` == 0.00) & (`BsmtFin SF 2` == 0.00) & (`Bsmt Unf SF` == 0.00) & (`Total Bsmt SF` == 0.00) & (`Bsmt Full Bath` == 0) & (`Bsmt Half Bath` == 0))'\n",
    "df.query(q1).query(q2)[basement_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- The previous entries show some kind of inconsistency, among the variables describing the basement facilities\n",
    "\n",
    "Solution:\n",
    "- Since these are only a few entries, we'll fix the entries using common sense\n",
    "- We'll populate the column `'Bsmt Exposure'` of entries 875, 1681 and 1726, where  indicates there is no basement but we there's a basement in Unfinished conditions, with `'Bsmt Exposure'=='No'` that indicates no exposure to outside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[602, ['BsmtFin Type 2']] = 'Unf'\n",
    "df.loc[[875,1681, 1726], ['Bsmt Exposure']] = 'No'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check for potential inconsistencies among the various surface features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inconsistent_basement_surface = (df['BsmtFin SF 1'] + df['BsmtFin SF 2'] + df['Bsmt Unf SF']) != df['Total Bsmt SF']\n",
    "inconsistent_basement_surface.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- The Basement surface features are consistent among for all entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Garage Variables__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garage_features = ['Garage Type', 'Garage Finish', 'Garage Cars', 'Garage Area', 'Garage Qual', 'Garage Cond']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we start by looking at entries where at least one feature indicates 'NA', but where this is inconsistent witht he other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = '(`Garage Type` == \"NA\") | (`Garage Finish` == \"NA\") | (`Garage Cars` == 0) | (`Garage Area` == 0) | (`Garage Qual` == \"NA\") & (`Garage Cond` == \"NA\")'\n",
    "q2 = '~((`Garage Type` == \"NA\") & (`Garage Finish` == \"NA\") & (`Garage Cars` == 0) & (`Garage Area` == 0) & (`Garage Qual` == \"NA\") & (`Garage Cond` == \"NA\"))'\n",
    "df.query(q1).query(q2)[garage_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- There is one entry with inconsistent values\n",
    "\n",
    "Solution:\n",
    "- We decide to drop this entry since it's just one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([1114], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Year Built vs Year Remod/Add vs Year Sold__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that the following conditions are verified:\n",
    "-  'Yr Sold' >= 'Year Remod/Add'\n",
    "- 'Year Remod/Add' >= 'Year Built' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Year Remod/Add'] < df['Year Built']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '(`Year Built` > `Yr Sold`)'\n",
    "df.query(q)[['Year Built', 'Year Remod/Add', 'Yr Sold']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:  \n",
    "- No inconsistencies observed between 'Year Built' and 'Year Remod/Add'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Misc Feature and Misc Val__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there is no 'Misc Feature' the corresponding 'Misc Val' should be 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = '(`Misc Feature` ==  \"NA\") & (`Misc Val` != 0)'\n",
    "df.query(q)[['Misc Feature', 'Misc Val']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:  \n",
    "- No inconsistencies observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Percentage of dropped entries: {(2430-df.shape[0])/2430*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll look at possible feature transformations than can improve the prediction power of our model. \n",
    "\n",
    "- We'll start by analysing the distribution of target variable __'Sale Price'__, which is also a very important part of our modelling\n",
    "- Then the the analysis will be split in 4 parts, one for each type of feature (nominal, ordinal, discrete and continuous). \n",
    "- In each part we'll start by looking at the relationship between the independent features and the target variable\n",
    "- We also compute correlation factors between independent features and target variable.\n",
    "- Feature encoding for the ordinal features will also be handled in this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next plot the the distribution of the target variable 'Sale Price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "fig, axs = plt.subplots(1,2, figsize=(10, 5))\n",
    "\n",
    "df['SalePrice'].hist(bins=50, ax=axs[0])\n",
    "axs[0].set_xlabel('SalePrice')\n",
    "axs[0].set_ylabel('Counts')\n",
    "\n",
    "saleprice_log = np.log(df['SalePrice'])\n",
    "\n",
    "# Center and scale the log-transformed target\n",
    "#scaler_target = StandardScaler()\n",
    "#saleprice_log_scaled = scaler_target.fit_transform(saleprice_log.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "df['SalePrice_transf'] = saleprice_log\n",
    "df['SalePrice_transf'].hist(bins=50, ax=axs[1])\n",
    "\n",
    "axs[1].set_xlabel('log(SalePrice)')\n",
    "axs[1].set_ylabel('Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- We observe that the distribution of the target variable resembles a skewed normal distribution\n",
    "  \n",
    "Solution:\n",
    "- By applying a log transformation (shown in right-hand side plot), the new distribution more closely resembles a normal distributione\n",
    "- We'll use the log transformed target variable for our model fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('SalePrice', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nominal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " __Relationship with target variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 5\n",
    "n_rows = 5\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(18, 12))\n",
    "\n",
    "for i, col in enumerate(nominal_features):\n",
    "    r, c = np.divmod(i, n_cols)\n",
    "    sns.boxplot(x=col, y='SalePrice_transf', data=df, ax=axs[r, c])\n",
    "    #axs[r, c].set_yscale('log')\n",
    "    axs[r, c].set_title(col, y=1.0)\n",
    "    axs[r, c].set_xlabel('')\n",
    "    for label in axs[r, c].get_xticklabels():\n",
    "        label.set_rotation(45)  # Rotate x-axis labels\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- The following features present a high number of categories: 'MS Subclass', 'Neighborhood', 'Exterior 1st', 'Exterior 2nd'. It could be interesting to group these in fewer categories as a function of 'Sale Price'\n",
    "- Some features present little relationship with target variable: 'Lot Config', 'Misc Feature'. Maybe these could be dropped\n",
    "\n",
    "Solution:\n",
    "- We apply the grouping strategy to 'MS SubClass', 'Neighborhood', 'Exterior 1st', 'Exterior 2nd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_bins = {}\n",
    "\n",
    "bins = 5\n",
    "highcardinality_features = ['MS SubClass', 'Neighborhood', 'Exterior 1st', 'Exterior 2nd']\n",
    "\n",
    "# Derive the the new binning to reduce the cardinality of the features\n",
    "for feature in highcardinality_features:\n",
    "    \n",
    "    # Calculate mean SalePrice for each category\n",
    "    mean_sale_price = df.groupby(feature)['SalePrice_transf'].mean()\n",
    "\n",
    "    # Bin the mean SalePrice into 4 groups\n",
    "    feature_bins = pd.qcut(mean_sale_price, q=bins, labels=[f\"bin_{i+1}\" for i in range(bins)])\n",
    "\n",
    "    df[feature] = df[feature].map(feature_bins)  # Use -1 or any fill value for missing mappings\n",
    "    df_test[feature] = df_test[feature].map(feature_bins)  # Use -1 or any fill value for missing mappings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 2\n",
    "n_rows = 2\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(13, 7))\n",
    "\n",
    "for i, col in enumerate(['MS SubClass', 'Neighborhood', 'Exterior 1st', 'Exterior 2nd']):\n",
    "    r, c = np.divmod(i, n_cols)\n",
    "    sns.boxplot(x=col, y='SalePrice_transf', data=df, ax=axs[r, c])\n",
    "    #axs[r, c].set_yscale('log')\n",
    "    axs[r, c].set_title(col, y=1.0)\n",
    "    axs[r, c].set_xlabel('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next adapt our lists of ordinal and nominal features to take into account the change of the variables we have just grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = ordinal_features + highcardinality_features\n",
    "nominal_features = list(set(nominal_features) - set(highcardinality_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing in advance that these features have an ordinal nature provided by the dataset documentation, we decide to start right way with implementing the Ordinal Encoding. We also treat here the 4 new ordinal features we have created in the previous section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Encoding__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we collect the encoding order for the ordinal features from the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_order = {\n",
    "    'Lot Shape': ['IR3', 'IR2', 'IR1', 'Reg'], \n",
    "    'Utilities': ['ELO', 'NoSeWa', 'NoSewr', 'AllPub'],\n",
    "    'Land Slope': ['Sev', 'Mod', 'Gtl'],\n",
    "    'Overall Qual': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'],\n",
    "    'Overall Cond': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10'],\n",
    "    'Exter Qual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'Exter Cond': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'Bsmt Qual': ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'Bsmt Cond': ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'Bsmt Exposure': ['NA', 'No', 'Mn', 'Av', 'Gd'], \n",
    "    'BsmtFin Type 1': ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "    'BsmtFin Type 2': ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'],\n",
    "    'Heating QC': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'Electrical': ['Mix', 'FuseP', 'FuseF', 'FuseA', 'SBrkr'],\n",
    "    'Kitchen Qual': ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'Functional': ['Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'],\n",
    "    'Fireplace Qu': ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'Garage Finish': ['NA', 'Unf', 'RFn', 'Fin'],\n",
    "    'Garage Qual': ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'Garage Cond': ['NA', 'Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'Paved Drive': ['N', 'P', 'Y'],\n",
    "    'Pool QC': ['NA', 'Fa', 'TA', 'Gd', 'Ex'],\n",
    "    'Fence': ['NA', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv'],\n",
    "    'MS SubClass': ['bin_1', 'bin_2', 'bin_3', 'bin_4', 'bin_5'],\n",
    "    'Neighborhood': ['bin_1', 'bin_2', 'bin_3', 'bin_4', 'bin_5'],\n",
    "    'Exterior 1st': ['bin_1', 'bin_2', 'bin_3', 'bin_4', 'bin_5'],\n",
    "    'Exterior 2nd': ['bin_1', 'bin_2', 'bin_3', 'bin_4', 'bin_5'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next apply the ordinal encoding with a ColumnTransformer, and also verify if the encoding was implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the encoded columns\n",
    "for feature, order in encoding_order.items():\n",
    "    # convert feature to category with specified order\n",
    "    df[feature] = pd.Categorical(df[feature], categories=order, ordered=True)\n",
    "    df_test[feature] = pd.Categorical(df_test[feature], categories=order, ordered=True)\n",
    "\n",
    "    # replace categories with numerical codes\n",
    "    df[feature] = df[feature].cat.codes\n",
    "    df_test[feature] = df_test[feature].cat.codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing a few entries with the input dataset we confirm that the encoding was applied correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relationship with target variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 5\n",
    "n_rows = 6\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(15, 10))\n",
    "\n",
    "for i, col in enumerate(ordinal_features):\n",
    "    r, c = np.divmod(i, n_cols)\n",
    "    sns.boxplot(x=col, y='SalePrice_transf', data=df, ax=axs[r, c])\n",
    "    #axs[r, c].set_yscale('log')\n",
    "    axs[r, c].set_title(col, y=1.0)\n",
    "    axs[r, c].set_xlabel('')\n",
    "    for label in axs[r, c].get_xticklabels():\n",
    "        label.set_rotation(45)  # Rotate x-axis labels\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- Feature 'Utilities' has a single category with entries and so can be dropped\n",
    "- For many features the correlation seems very weak (e.g. 'Land Slope', 'Fence', etc.)\n",
    "- For the features where there seems to be a stronger dependency, this seems to be compatible with a linear relationship which is compatible with the linear encoding we have just applied \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Correlations with target variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation of ordinal features with SalePrice\n",
    "ordinal_corr = df[ordinal_features].corrwith(df['SalePrice_transf']).abs().sort_values(ascending=False)\n",
    "print(\"Correlation of Ordinal Features SalePrice:\")\n",
    "print(ordinal_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- The features with highest predictin power > 0.5 are:\n",
    "    - __'Overall Cond'__\n",
    "    - __'Exter Qual'__\n",
    "    - __'Kitchen Qual'__\n",
    "    - __'Bsmt Qual'__\n",
    "    - __'Garage Finish'__\n",
    "    - __'Fireplace Qu'__\n",
    "     \n",
    "- The following features show a very low correlation with 'Sale Price' < 0.1: \n",
    "    - __'Utilities'__: should be dropped in any case since only one category is populated\n",
    "    - __'Exter Cond'__\n",
    "    - __'BsmtFin Type 2'__\n",
    "    - __'Land Slope'__\n",
    "    - __'Overall Cond'__\n",
    "    - __'Pool QC'__: this provides a big effect but only when = 4 (Excellent pool quality)\n",
    "\n",
    "Solution:  \n",
    "- Drop __'Utilities'__,  __'Exter Cond'__, __'BsmtFin Type 2'__, __'Land Slope'__, __'Overall Cond'__.\n",
    "- Replace __'Pool QC'__ with a binary new feature indicating whether a pool with excellent quality is present or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Utilities', 'Exter Cond', 'BsmtFin Type 2', 'Land Slope', 'Overall Cond'], axis=1, inplace=True)\n",
    "df_test.drop(['Utilities', 'Exter Cond', 'BsmtFin Type 2', 'Land Slope', 'Overall Cond'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HasPool_Excellent'] = (df['Pool QC'] == 'Ex')\n",
    "df_test['HasPool_Excellent'] = (df_test['Pool QC'] == 'Ex')\n",
    "\n",
    "df.drop('Pool QC', axis=1, inplace=True)\n",
    "df_test.drop('Pool QC', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_features = list(set(ordinal_features) - set(['Utilities', 'Exter Cond', 'BsmtFin Type 2', 'Land Slope', 'Overall Cond', 'Pool QC']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relationship with target variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 4\n",
    "n_rows = 4\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(12,10))\n",
    "\n",
    "for i, col in enumerate(discrete_features):\n",
    "    r, c = np.divmod(i, n_cols)\n",
    "    sns.scatterplot(x=col, y='SalePrice_transf', data=df, ax=axs[r, c])\n",
    "    #axs[r, c].set_yscale('log')\n",
    "    axs[r, c].set_title(col, y=1.0)\n",
    "    axs[r, c].set_xlabel('')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Correlation with target variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation of Numerical Discrete features with SalePrice:\")\n",
    "df[discrete_features].corrwith(df['SalePrice_transf']).abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- __'Yr Sold'__, __'Bsmt Half Bath'__ and __'Mo Sold'__ present a very weak correlation\n",
    "\n",
    "Solution:  \n",
    "- Drop __'Yr Sold'__, __'Mo Sold'__ and __'Bsmt Half Bath'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Yr Sold', 'Mo Sold', 'Bsmt Half Bath'], axis=1, inplace=True)\n",
    "df_test.drop(['Yr Sold', 'Mo Sold', 'Bsmt Half Bath'], axis=1, inplace=True)\n",
    "discrete_features = list(set(discrete_features) - set(['Yr Sold', 'Mo Sold', 'Bsmt Half Bath']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relationship with target variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 4\n",
    "n_rows = 5\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(12,10))\n",
    "\n",
    "for i, col in enumerate(continuous_features):\n",
    "    r, c = np.divmod(i, n_cols)\n",
    "    sns.scatterplot(x=col, y='SalePrice_transf', data=df, ax=axs[r, c])\n",
    "    axs[r, c].set_title(col, y=1.0)\n",
    "    axs[r, c].set_xlabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Correlation with target variable__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = df[continuous_features].corrwith(df['SalePrice_transf']).abs().sort_values(ascending=False)\n",
    "print(\"Correlation of numerical continuous features with SalePrice:\")\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__New Features__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "\n",
    "- We have previously said, when observing the histograms of these features, that for many to be replaced with binary feature, since they had most values at zero. \n",
    "- Now we confirm that most of these have they very week correlation with __'SalePrice'__\n",
    "- However, for the following ones (__'2nd Flr SF'__,  __'Wood Deck SF'__, __'Open Porch SF'__, __'Mas Vnr Area'__) the correlation is not negligeble.\n",
    "\n",
    "Solution:\n",
    "- Create binary features from features which have most values at zero\n",
    "- Drop the ones with very weak correlation and keep the others: \n",
    "    - Binarize and drop:\n",
    "        - __'BsmtFin SF 2'__\n",
    "        - __'Misc Val'__\n",
    "        - __'3Ssn Porch'__\n",
    "        - __'Low Qual Fin Sf'__\n",
    "        - __'Screen Porch'__\n",
    "        - __'Pool Area'__\n",
    "        - __'Enclosed Porch'__\n",
    "    - Binarize and keep:\n",
    "        - __'2nd Flr SF'__\n",
    "        - __'Wood Deck SF'__\n",
    "        - __'Open Porch SF'__\n",
    "        - __'Mas Vnr Area'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to binarize and drop\n",
    "binarize_drop_features = ['BsmtFin SF 2', 'Misc Val', '3Ssn Porch', 'Low Qual Fin SF', 'Screen Porch', 'Pool Area', 'Enclosed Porch']\n",
    "\n",
    "# Columns to binarize and keep\n",
    "binarize_keep_features = ['2nd Flr SF', 'Wood Deck SF', 'Open Porch SF', 'Mas Vnr Area']\n",
    "\n",
    "# Binarize the columns\n",
    "for feature in binarize_drop_features:\n",
    "    df[feature+'_binary'] = (df[feature] > 0).astype(int)\n",
    "    df_test[feature+'_binary'] = (df_test[feature] > 0).astype(int)\n",
    "    \n",
    "    df.drop(feature, axis=1, inplace=True)\n",
    "    df_test.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "for feature in binarize_keep_features:\n",
    "    df[feature+'_binary'] = (df[feature] > 0).astype(int)    \n",
    "    df_test[feature+'_binary'] = (df_test[feature] > 0).astype(int)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = list(set(continuous_features) - set(binarize_drop_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still need to encode the nominal features. This step will be performed during the model creating step using one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the df dataset into training and validation sets\n",
    "# Define the target variable and features\n",
    "X = df.drop(columns='SalePrice_transf', axis=1).copy()\n",
    "y = df['SalePrice_transf']\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model prediction is simply the mean of the 'Sale Price' for all entries, and does not depend on any of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "baseline_model = LinearRegression()\n",
    "\n",
    "# Reshape target mean to fit the LinearRegression format (as a single feature)\n",
    "pred_baseline_val_log = np.full_like(y_val, fill_value=np.mean(y_train), dtype=float)\n",
    "\n",
    "#pred_baseline_val_log = scaler_target.inverse_transform(pred_baseline_val_transf.reshape(-1, 1)).flatten()\n",
    "\n",
    "pred_baseline_val = np.exp(pred_baseline_val_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- The 3 variables with the highest correlation values with 'SalePrice_log' are:\n",
    "    - __'Overall Qual'__: 0.83\n",
    "    - __'Gr Liv Area'__: 0.73\n",
    "    - __'Garage Cars'__: 0.68\n",
    "\n",
    "Solution:  \n",
    "- Use these features in the simple model as may provide higehet prediction power\n",
    "- A StandardScaling step is implemented as preprocessing step in order to make sure all features contribution are on the same scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "\n",
    "# Set up transformers\n",
    "onehot_encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "ordinal_encoder = OrdinalEncoder(categories=encoding_order)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale', scaler, ['Overall Qual','Gr Liv Area','Garage Cars'])  \n",
    "    ],\n",
    "    remainder='passthrough' \n",
    ")\n",
    "\n",
    "# Set up the pipeline with preprocessing and the linear regression model\n",
    "pipe_simple = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = X_train[['Overall Qual', 'Gr Liv Area', 'Garage Cars']]\n",
    "y = y_train  # Replace 'SalePrice' with the actual target variable name\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipe_simple.fit(X, y)\n",
    "\n",
    "# Make predictions on the validation data, and apply inverse transformation\n",
    "pred_simple_val = np.exp(pipe_simple.predict(X_val[['Overall Qual', 'Gr Liv Area', 'Garage Cars']]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model we select the features that present the highest correlation factors with the target variable. We additionally select nominal features which visually seem to provide for bigger prediction power. A one-hot encoding is applied to the nominal features.\n",
    "\n",
    "Below the list of variables to use:\n",
    "\n",
    "- Nominal Features\n",
    "    - MS Zoning\t\n",
    "    - Condition 2\n",
    "    - Roof Matl\n",
    "    - Foundation\n",
    "    - Heating\n",
    "    - Central Air\n",
    "\n",
    "- Ordinal Features\n",
    "    - Overall Qual:     0.80\n",
    "    - Neighborhood:     0.71\n",
    "    - Exter Qual:       0.71\n",
    "    - Kitchen Qual:     0.68\n",
    "    - Bsmt Qual:        0.61\n",
    "    - Garage Finish:    0.56\n",
    "    - Fireplace Qu:     0.54\n",
    "\n",
    "\n",
    "- Discrete Features\n",
    "    - Garage Cars:      0.68\n",
    "    - Year Built:       0.63\n",
    "    - Full Bath:        0.59\n",
    "    - Year Remod/Add:   0.58\n",
    "    - TotRms AbvGrd:    0.50\n",
    "    - Fireplaces:       0.50\n",
    "\n",
    "\n",
    "- Continuous Features\n",
    "    - Gr Liv Area:      0.73\n",
    "    - Total Bsmt SF:    0.66\n",
    "    - Garage Area:      0.65\n",
    "    - 1st Flr SF:       0.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define selected features based on correlation and prediction power\n",
    "intermediate_nominal_features = ['MS Zoning', 'Condition 2', 'Roof Matl', 'Foundation', 'Heating', 'Central Air']\n",
    "intermediate_ordinal_features = ['Overall Qual', 'Neighborhood', 'Exter Qual', 'Kitchen Qual', 'Bsmt Qual', 'Garage Finish', 'Fireplace Qu']\n",
    "intermediate_discrete_features = ['Garage Cars', 'Year Built', 'Full Bath', 'Year Remod/Add', 'TotRms AbvGrd', 'Fireplaces']\n",
    "intermediate_continuous_features = ['Gr Liv Area', 'Total Bsmt SF', 'Garage Area', '1st Flr SF']\n",
    "\n",
    "# Set up transformers\n",
    "onehot_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore', drop='first')  # Drop first to avoid multicollinearity\n",
    "ordinal_encoder = OrdinalEncoder(categories=encoding_order)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', onehot_encoder, intermediate_nominal_features),        # One-hot encode nominal feature\n",
    "        ('scale', scaler, intermediate_discrete_features + intermediate_continuous_features)  # Scale continuous and discrete features\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns unchanged\n",
    ")\n",
    "\n",
    "# Set up the pipeline with preprocessing and the linear regression model\n",
    "pipe_intermediate = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Separate features and target variable\n",
    "X = X_train[intermediate_nominal_features + intermediate_ordinal_features + intermediate_discrete_features + intermediate_continuous_features]\n",
    "y = y_train  # Replace 'SalePrice' with the actual target variable name\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipe_intermediate.fit(X, y)\n",
    "\n",
    "# Make predictions on the test data\n",
    "pred_intermediate_val_log = pipe_intermediate.predict(X_val[intermediate_nominal_features + intermediate_ordinal_features + intermediate_discrete_features + intermediate_continuous_features])\n",
    "\n",
    "# Apply inverse transformation to get the actual SalePrice\n",
    "#pred_intermediate_val_log = scaler_target.inverse_transform(pred_intermediate_val_transf.reshape(-1, 1)).flatten()\n",
    "pred_intermediate_val = np.exp(pred_intermediate_val_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing a more complex model taking into account all of the features in the available dataset, one much realize many of the features provide redundant or linear dependent information. For example, features like 'Total Bsmt SF' and 'Gr Liv Area' are likely correlated as they both measure areas of the house. Redundant features may potentially generate overfitting problems, and produce models worse at generalizing in unkown data. \n",
    "\n",
    "Regularization techniques, such Ridge regression, help mitigate this issue. Helping to fight overfitting and to improve the generalizability of a model. \n",
    "\n",
    " Ridge regression (L2 Regularization) penalizes large model coefficients, which helps prevent overfitting, by adding a penalyt term to the cost function which is proportional to the square of the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Set up transformers\n",
    "onehot_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')  # Drop first to avoid multicollinearity\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Define the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('onehot', onehot_encoder,  nominal_features),              # One-hot encode nominal feature\n",
    "        ('scale', scaler, discrete_features + continuous_features)  # Scale continuous and discrete features\n",
    "    ],\n",
    "    remainder='passthrough'  # Pass through any other columns unchanged\n",
    ")\n",
    "\n",
    "alpha = 45.35\n",
    "\n",
    "# Set up the pipeline with preprocessing and the linear regression model\n",
    "pipe_complex = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(alpha=alpha))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipe_complex.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "pred_complex_val_log = pipe_complex.predict(X_val)\n",
    "\n",
    "# Apply inverse transformation to get the actual SalePrice\n",
    "#pred_complex_val_log = scaler_target.inverse_transform(pred_complex_val_transf.reshape(-1, 1)).flatten()\n",
    "pred_complex_val = np.exp(pred_complex_val_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, r2_score#, root_mean_squared_error\n",
    "\n",
    "def evaluate_metric(y_pred, y_test):\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    #rms = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Print MAE in dollars\n",
    "    print(f\" - MAE: {mae/1000:.3f} thousand dollars\")\n",
    "    #print(f\"Root Mean Squared Error (RMSE): {rms/1000:.3f} dollars\")\n",
    "    print(f\" - R^2: {r2:.2f}\")\n",
    "    \n",
    "    return mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_val_true = np.exp(scaler_target.inverse_transform(y_val.values.reshape(-1, 1)).flatten())\n",
    "y_val_true = np.exp(y_val.values.reshape(-1, 1).flatten())\n",
    "\n",
    "\n",
    "print(f'Baseline Model (Mean value):')\n",
    "mae_baseline, r2_baseline = evaluate_metric(pred_baseline_val, y_val_true)\n",
    "print(f'Simple Model (Linear Regression):')\n",
    "mae_simple, r2_simple = evaluate_metric(pred_simple_val, y_val_true)\n",
    "print(f'Intermediate Model (Linear Regression):')\n",
    "mae_intermediate, r2_intermediate = evaluate_metric(pred_intermediate_val, y_val_true)\n",
    "print(f'Complex Model (Ridge Regression alpha={alpha}):')\n",
    "mae_complex, r2_complex = evaluate_metric(pred_complex_val, y_val_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alpha parameter in Ridge regression is an hyperparameter, which needs to be chosen by us and is not a learnt parameter. For this we'll perform a grid search by fitting and evaluating different models for varying values of alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable to store the results\n",
    "gs_results = []\n",
    "\n",
    "# Grid search\n",
    "for alpha in np.logspace(-4, 4, num=100):\n",
    "    # Create/fit the pipeline\n",
    "    pipe_complex_reg = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(alpha))\n",
    "])\n",
    "    pipe_complex_reg.fit(X_train, y_train)\n",
    "\n",
    "    # Save model and its performance on train/test sets\n",
    "    gs_results.append(\n",
    "        {\n",
    "            \"alpha\": alpha,\n",
    "            \"train_mae\": mean_absolute_error(y_train, pipe_complex_reg.predict(X_train)),\n",
    "            \"val_mae\": mean_absolute_error(y_val, pipe_complex_reg.predict(X_val)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Convert results to DataFrame\n",
    "gs_results = pd.DataFrame(gs_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we plot the validation curves. For each alpha we plot the MAE score of the train a validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the validation curves\n",
    "plt.semilogx(gs_results[\"alpha\"], gs_results[\"train_mae\"], label=\"train curve\")\n",
    "plt.semilogx(gs_results[\"alpha\"], gs_results[\"val_mae\"], label=\"val curve\")\n",
    "plt.xlabel(\"$alpha$\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Alpha value for minimum in validation set MAE curve:\")\n",
    "print(f\"alpha = {gs_results['alpha'].iloc[gs_results['val_mae'].idxmin()]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:  \n",
    " \n",
    "- At low values of alpha, there is a wider gap between the training and validation curves, which suggests overfitting of the model.\n",
    "- At alpha=45.35 the validation curve reached its minimum which should correspond to the optimal tradeoff between model accuracy and generalization capability.\n",
    "- As alpha increases, from alpha=100, there's a steep loss of prediction power of the model, indicating significant underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for the models\n",
    "models = [\"Baseline\", \"Simple\", \"Intermediate\", \"Complex\"]\n",
    "mae_values = [mae_baseline, mae_simple, mae_intermediate, mae_complex]  # in thousand dollars\n",
    "r2_values = [r2_baseline, r2_simple, r2_intermediate, r2_complex]\n",
    "\n",
    "# Plot 1: MAE for each model\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(models, mae_values)\n",
    "plt.ylabel(\"MAE (in thousand dollars)\")\n",
    "plt.title(\"Mean Absolute Error (MAE) for Models\")\n",
    "\n",
    "# Plot 2: R^2 for each model\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(models, r2_values, color='orange')\n",
    "plt.ylabel(\"R^2 Score\")\n",
    "plt.title(\"R^2 Score for Models\")\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous plot demonstrates the performance progression across four models, with clear improvements as model complexity increases. \n",
    "\n",
    "- The Baseline Model, which uses only the mean value as a prediction, shows the highest Mean Absolute Error (MAE) at approximately 54 thousand dollars and a negative R^2 score, indicating that it fails to capture any meaningful pattern in the data.\n",
    "\n",
    "- The Simple Model using basic linear regression with independent variables __'Overall Qual'__, __'Gr Liv Area'__ and __'Garage Cars'__, reduces the MAE significantly to about 22.5 thousand dollars and achieves a strong R^2=0.82, demonstrating that even a model with very few variables, but which present a high correlation with target variable, can capture much of the data's variance.\n",
    "\n",
    "- The Intermediate Model adds additional features, in a total of 23 of all different datatypes, presenting the highest correlation with target variable. This achieves a MAE of around 16.8 thousand dollars and an improved R^2 = 0.89. This enhancement shows the value of adding relevant predictors to the model.\n",
    "\n",
    "- Finally, the Complex Model, which uses Ridge regression with optimized regularization (alpha = 25.95), achieves the lowest MAE at 14.3 thousand dollars and the highest R^2=0.92. This improvement suggests that using all features, including many that are correlated together with regularization effectively improves the model while mitigating overfitting, leading to better generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_simple_test = np.exp(pipe_simple.predict(df_test[['Overall Qual', 'Gr Liv Area', 'Garage Cars']]))\n",
    "\n",
    "df_combined = pd.concat([df_test_pid.reset_index(drop=True), pd.DataFrame(pred_simple_test, columns=['SalePrice'])], axis=1)\n",
    "df_combined = df_combined.astype(int)\n",
    "df_combined.to_csv('predictions-simple-model.csv', index=False)\n",
    "df_combined.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first check for our data processing and prediction, we show next the histogram distribution for all 3 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_simple_train = np.exp(pipe_simple.predict(X_train[['Overall Qual', 'Gr Liv Area', 'Garage Cars']]))\n",
    "\n",
    "plt.hist([pred_simple_train, pred_simple_val, pred_simple_test], bins=50, alpha=0.5, label=['Train', 'Valid', 'Test'])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 'Overall Qual' for df\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 8))\n",
    "\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].hist(df['Overall Qual'], bins=10)\n",
    "axes[0].set_title('Distribution in df')\n",
    "axes[1].hist(df_test['Overall Qual'], bins=10)\n",
    "axes[1].set_title('Distribution in df_test')\n",
    "\n",
    "axes[2].hist(df['Gr Liv Area'], bins=10)\n",
    "axes[2].set_title('Distribution in df')\n",
    "axes[3].hist(df_test['Gr Liv Area'], bins=10)\n",
    "axes[3].set_title('Distribution in df_test')\n",
    "\n",
    "axes[4].hist(df['Garage Cars'], bins=10)\n",
    "axes[4].set_title('Distribution in df')\n",
    "axes[5].hist(df_test['Garage Cars'], bins=10)\n",
    "axes[5].set_title('Distribution in df_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermediate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_intermediate_test = np.exp(pipe_intermediate.predict(df_test))\n",
    "\n",
    "df_combined = pd.concat([df_test_pid.reset_index(drop=True), pd.DataFrame(pred_intermediate_test, columns=['SalePrice'])], axis=1)\n",
    "df_combined = df_combined.astype(int)\n",
    "df_combined.to_csv('predictions-intermediate-model.csv', index=False)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_complex_test = np.exp(pipe_complex.predict(df_test))\n",
    "\n",
    "df_combined = pd.concat([df_test_pid.reset_index(drop=True), pd.DataFrame(pred_complex_test, columns=['SalePrice'])], axis=1)\n",
    "df_combined = df_combined.astype(int)\n",
    "df_combined.to_csv('predictions-complex-model.csv', index=False)\n",
    "df_combined.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
